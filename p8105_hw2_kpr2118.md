Homework 2
================
Kailey Rishovd
9/30/2020

``` r
library(tidyverse)
```

    ## -- Attaching packages ---------------------------------------------------------- tidyverse 1.3.0 --

    ## v ggplot2 3.3.2     v purrr   0.3.4
    ## v tibble  3.0.3     v dplyr   1.0.2
    ## v tidyr   1.1.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts ------------------------------------------------------------- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(readxl)
```

## Problem 1

Define a path to the dataset.

``` r
path_to_trash_data = "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx"
```

Read the Mr. Trashwheel dataset.

``` r
trashwheel_df = 
  read_xlsx(
    path = path_to_trash_data,
    sheet = "Mr. Trash Wheel",
    range = cell_cols("A:N")) %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(
    sports_balls = round(sports_balls), 
    sports_balls = as.integer(sports_balls)
  )
```

Read precipitation data for 2018 and 2017\!

``` r
precip_2018 = 
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
  sheet = "2018 Precipitation",
  skip = 1 
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018) %>% 
  relocate(year)

precip_2017 = 
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
  sheet = "2017 Precipitation",
  skip = 1 
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2017) %>% 
  relocate(year)
```

Now combine annual precipitation. Made ‘helper’ tibble for numeric and
character representation of month, then merge with month number key,
with the precipitation dataset.

``` r
month_df = 
  tibble(
    month = 1:12, 
    month_name = month.name
  )

precip_df = 
  bind_rows(precip_2018, precip_2017)

precip_df = 
  left_join(precip_df, month_df, by = "month") 

precip_df = 
  precip_df %>% 
  select(-month) %>% 
  select(year, month = "month_name", total)
```

This dataset contains information from the Mr. Trashwheel trash
collector in Baltimore, Maryland. As trash enters the inner harbor, the
trashwheel collects that trash, and stores it in a dumpster. The dataset
contains information on year, month, and trash collected, including some
specific kinds of trash.There are a total of 344 rows in our final
dataset. Additional data sheets include monthly precipitation data. In
this dataset:

  - The total precipitation in 2018 was 70.33 inches.

  - The median number of sports balls found in a dumpster in 2017 was 8.

## Problem 2

Read the NYC Transit dataset and begin to clean.

``` r
transit_df = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(entry = recode(entry, YES = "TRUE", NO = "FALSE")) %>% 
  mutate(entry = as.logical(entry))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_double(),
    ##   Route9 = col_double(),
    ##   Route10 = col_double(),
    ##   Route11 = col_double(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

This dataset contains information from the NYC Transit Subway Entrance
and Exit data. It has information on each line including stations,
station locations, routes served, entrance type, entrance availability,
vending availablity, and ada compliance. There are a total of 1868 rows
and 19 columns in this dataset. So far, I have cleaned up names,
selected variables of interest, and converted the ‘entry’ variable from
character into a logical variable. The data are relatively tidy but
there could be more done to make it even more so… possibilities include
organizing the arrangement of information or making some variables more
distinct. In this dataset:

  - There are 465 distinct stations.

  - 84 stations are ADA compliant.

  - 183 entrance/exists do not have vending and 69 allow entry …
    therefore, 37.704918 percent of station entrances/exits without
    vending allow entrance.

Now I will reformat route number and route name into distinct variables.

``` r
transit_df = 
  transit_df %>% 
  mutate_at(vars(route1:route11), as.character) %>% 
  pivot_longer(
    route1:route11, 
    names_to = "route_number",
    names_prefix = "route",
    values_to = "route_name",
  ) %>% 
  drop_na(route_name) %>% 
  relocate(line:station_longitude, route_name, route_number, entrance_type:ada)
```

Now we can read things such as…

  - 60 distinct stations serve the A train

  - Of the stations that serve the A train, 17 are ADA compliant.

## Problem 3

Read in and clean selected FiveThirtyEight datasets…

``` r
pols_df = 
  read_csv("./data/fivethirtyeight_datasets/pols-month.csv") %>% 
  separate(mon, into = c("year", "month", "day")) %>% 
  mutate_at(vars(year:day), as.integer)   
```

    ## Parsed with column specification:
    ## cols(
    ##   mon = col_date(format = ""),
    ##   prez_gop = col_double(),
    ##   gov_gop = col_double(),
    ##   sen_gop = col_double(),
    ##   rep_gop = col_double(),
    ##   prez_dem = col_double(),
    ##   gov_dem = col_double(),
    ##   sen_dem = col_double(),
    ##   rep_dem = col_double()
    ## )

``` r
polsmonth_df =
    tibble(
      month = 1:12,
      month_name = month.name
    )

pols_df = 
    left_join(pols_df, polsmonth_df, by = "month")  

pols_df = 
  pols_df %>% 
  select(-month) %>% 
  select(year, month = "month_name", day:rep_dem) %>% 
  mutate(
    president = case_when( 
      prez_dem == 1 ~ "dem",
      prez_gop == 1 ~ "gop",
      prez_gop == 2 ~ "gop",
    )
  )

pols_df = 
  pols_df %>% 
  select(year, month, president, gov_gop:rep_gop, gov_dem:rep_dem)
```

``` r
snp_df = 
  read_csv("./data/fivethirtyeight_datasets/snp.csv") %>% 
  separate(date, into = c("month", "day", "year")) %>% 
  mutate_at(vars(month:year), as.integer) 
```

    ## Parsed with column specification:
    ## cols(
    ##   date = col_character(),
    ##   close = col_double()
    ## )

``` r
snpmonth_df = 
    tibble(
      month = 1:12,
      month_name = month.name
    )

snp_df =  
    left_join(snp_df, snpmonth_df, by = "month") 

snp_df = 
  snp_df %>%  
  select(-month) %>% 
  select(year, month = "month_name", close)
```

``` r
unemployment_df = 
  read_csv("./data/fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names() %>% 
  mutate_at(vars(year), as.integer) %>% 
  pivot_longer(
    jan:dec, 
    names_to = "month", 
    values_to = "percent_unemp"
  ) %>% 
  mutate(month = recode(month, jan = "January", feb = "February", mar = "March", apr = "April", may = "May", jun = "June", jul = "July", aug = "August", sep = "September", oct = "October", nov = "November", dec = "December"))
```

    ## Parsed with column specification:
    ## cols(
    ##   Year = col_double(),
    ##   Jan = col_double(),
    ##   Feb = col_double(),
    ##   Mar = col_double(),
    ##   Apr = col_double(),
    ##   May = col_double(),
    ##   Jun = col_double(),
    ##   Jul = col_double(),
    ##   Aug = col_double(),
    ##   Sep = col_double(),
    ##   Oct = col_double(),
    ##   Nov = col_double(),
    ##   Dec = col_double()
    ## )

Now that all the datasets of interest are cleaned up, we can merge as
desired…

``` r
polssnp_df = 
  left_join(pols_df, snp_df, by = c("year", "month"))

final_df = 
  left_join(polssnp_df, unemployment_df, by = c("year", "month"))
```

The data pulled from FiveThirtyEight are the “pols-month”, “snp”, and
“unemployment” files. “Pols-month” contains information on the history
of national politicians, indicating democratic and republican
presidents, governors, senators, and representatives, at the given
times. “Snp” includes the closing values of Standard & Poor’s stock
market index at various dates of observation. The “unemployment” file
indicates the percentage of unemployment at given points in history
(month and year). From these datasets, after tidying up, I was able to
merge pols-month data with snp data, and then, from that merged datset,
merge with the unemployment data of interest. The “final\_df” dataset
includes information from all 3 files to show the intersection of
politics, the stock market, and unemployment.

The final dataset has a total of 822 rows and 11 columns. Information
includes data for the range of years from 1947 to 2015, and key
variables are year, month, president (democrat or republican in office),
close (the closing value of S\&P stock index), and precent\_unemp (the
percentage unemployed at that time).
